{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q12N0gg1hsOk"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers optuna #optuna used for automatic greed search for LightGBM modle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import optuna"
      ],
      "metadata": {
        "id": "uly-YTr8h-tJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load data\n",
        "df = pd.read_csv('synthetic_name_variations_1500_1.csv')\n",
        "# Ensure it has: 'name_input','variation','label' (0 or 1)\n",
        "\n",
        "# 2. (Optional) Basic text normalization\n",
        "# For Arabic you might strip tashkeel (diacritics) etc., but here we skip\n",
        "\n",
        "# 3. Load one multilingual embedding model\n",
        "#model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "bi_encoder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')  # For embeddings\n",
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')  # For similarity scoring"
      ],
      "metadata": {
        "id": "1C4XCDa6h_I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset will be composed of embeddings of input_name and variant , semantic similarity calculated with the crossEncoder , cosine / jaccard /manhattan metrics\n",
        "# 4. Generate Features\n",
        "def generate_features(names, variations):\n",
        "    # Embeddings\n",
        "    emb_names = bi_encoder.encode(names, show_progress_bar=True)\n",
        "    emb_vars = bi_encoder.encode(variations, show_progress_bar=True)\n",
        "\n",
        "    # Similarity metrics\n",
        "    cos_sim = util.cos_sim(emb_names, emb_vars).numpy().diagonal()\n",
        "    manhattan = np.sum(np.abs(emb_names - emb_vars), axis=1)\n",
        "    jaccard = [len(set(n.split()) & set(v.split())) / max(1, len(set(n.split()) | set(v.split())))\n",
        "              for n, v in zip(names, variations)]\n",
        "\n",
        "    # Cross-Encoder scores\n",
        "    cross_scores = cross_encoder.predict(list(zip(names, variations)))\n",
        "\n",
        "    return np.column_stack([emb_names, emb_vars, cos_sim, manhattan, jaccard, cross_scores])\n",
        "\n",
        "X = generate_features(df['input_name'].tolist(), df['variation'].tolist())\n",
        "y = df['label'].values"
      ],
      "metadata": {
        "id": "qqUgMOlpiBTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "lMjndjpniRyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    # Suggest hyperparameters\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'binary_logloss',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "    }\n",
        "\n",
        "    # Train LightGBM\n",
        "    model = lgb.LGBMClassifier(**params)\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_test, y_test)]\n",
        "\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "yol8blusiTzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=30)\n",
        "\n",
        "# Best hyperparameters\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(f\"  Accuracy: {trial.value:.4f}\")\n",
        "print(\"  Params: \", trial.params)"
      ],
      "metadata": {
        "id": "E-4KDeZOiVxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize with best params\n",
        "best_params = study.best_params\n",
        "best_params['objective'] = 'binary'  # Ensure this is set\n",
        "\n",
        "final_model = lgb.LGBMClassifier(**best_params)\n",
        "final_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "y_pred = final_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "KN8gNAWIjEil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix # Import confusion_matrix directly\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "jCf8pqnZjOad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fFyBCTVkjVgV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}